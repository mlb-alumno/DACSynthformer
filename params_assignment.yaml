experiment: "mini_assignment"

data_dir: "new_data/clean_train_dacs"
data_frames: "new_data/dac_train.xlsx"
validator_data_dir: "new_data/clean_validation_dacs"
validator_data_frames: "new_data/dac_validation.xlsx"


TransformerClass: "RopeCondDACTransformer" 
vocab_size: 1024 # dont mess with this
num_tokens: 4 #codeblocks

cond_params: 1 #1 (not counting the classes) not touch this!
model_size: 128 # must be divisible by num_heads , you can change this! 128 works well

Ti: 86 # 172 #86 #size of the inference sliding window and mask
Tt: 430 # must match the length of the sequences in the batch # length of the training data 430 samples 
batch_size: 4   #**


num_layers: 2 #**  , number of transformer layers. Originally 2
num_heads: 16 #8 # 8 # embed_size must be divisible by num_heads. Originally 8
forward_expansion: 2 #4 #4 
dropout_rate: 0.2
learning_rate: 0.0005

num_epochs: 300 ### 800 you can try 200 or 300

ErrorLogRate: 2 #2 ### 10 # how often we log the errors 
checkpoint_interval: 300 ###50 # 25 #how often we make a checkpoint. Originally 100

